#!/usr/bin/env python3
# -*- encoding: utf-8 -*-
""" implore.py - An even more desperate way to make requests for Humans.â„¢ """
""" GOALS:
    By creating this app, I wanted to answer these questions:

    - What are the internet activities that I use or want most often?
    - What are the things that are feasible, but that I cannot easily do?

    Here is a list of items I came up with. I have solved them to some degree, but any advice or suggestions is welcome. See Contributors document for information.

    - Get a webpage
    - Modify CSS
    - Convert to a 3d object
        - front is main page
        - shape / depth controlled by varying functions
    - Collect links, images, tags, or something else from webpages
    - Create a 'map' of link destinations
    - Create an image catalog for a specific site(with CSS)
    - Interact with underlying database
    - Disable certain JS apps
    - Disable or interact with certain trackers
    - Watch for cookies and other local data / permissions
    - Block specific activities of specific sites
    - Search for English (or language) words and separate them from tags
    - Analyze metadata
    - Count types of tags and metadata
    - Analyze or avoid ads
    - Test for API functionality
    """

# 'Standard Library'
import cgitb
import html.parser as html_parser
import os
import sys
import traceback

from collections import Counter, deque
from dataclasses import Field, dataclass, field
from datetime import datetime
from pprint import pprint
from time import gmtime, localtime, perf_counter, process_time, sleep, time

# 'third party'
import requests

# 'package imports'
from apscheduler.schedulers.blocking import BlockingScheduler
from bs4 import BeautifulSoup

from typing import Any, Dict, List, MutableSequence

try:
    import ujson as json  # faster json if available
except:
    import json

DEFAULT_PARSER: str = ""
XML: bool = False  # whether XML parsing is available at all
XML_PARSER: str = ""

SET_DEBUG: bool = True  # turn on for Dev Debugging
if SET_DEBUG:  # enable detailed logging
    # cgitb.enable([display[, logdir[, context[, format]]]])
    DEFAULT_CGITB_DISPLAY: int = 1  # 0 to suppress messages
    DEFAULT_CGITB_FILE_LOGDIR: str = os.path.join(os.path.dirname(__file__),
                                                  "LOGS")
    DEFAULT_CGITB_CONTEXT: int = 3  # 5 is the normal default
    DEFAULT_CGITB_FORMAT: str = "html"

    cgitb.enable(
        logdir=DEFAULT_CGITB_FILE_LOGDIR,
        display=DEFAULT_CGITB_DISPLAY,
        context=DEFAULT_CGITB_CONTEXT,
        format=DEFAULT_CGITB_FORMAT,
    )

    DEFAULT_CGITB_ERROR: sys.exc_info = ()


def get_parser():
    # setup html parser
    try:  # use <lxml> for speed (HTML and / or XML) (VERY fast)
        import lxml as parser
        DEFAULT_PARSER = "lxml"
        XML = True
        XML_PARSER = "xml"
        return parser
    except ImportError:
        try:  # use <html5lib> for accuracy (VERY slow)
            import html5lib as parser
            DEFAULT_PARSER = "html5lib"
            return parser
        except ImportError:  # use <html5lib> Python's built-in html parser
            import html.parser as parser
            DEFAULT_PARSER = "html.parser"
            return parser


parser = get_parser()



@dataclass
class WebParser(html_parser):
    def __post_init__(self):
        print(DEFAULT_PARSER)


if True:  # Default constants
    # * time zones and scheduling
    USE_TZ: bool = False
    sch = BlockingScheduler()

    # * html and parsing
    DEFAULT_ENCODING: str = "UTF-8"
    url: str = ""
    script_text: str = ""
    current_number: str

    now: struct_time = gmtime()


def db_print(*args, file=sys.stderr, **kwargs):
    """ Print messages to STDERR in debug mode (if <SET_DEBUG> is set) """
    if SET_DEBUG:
        print("@db ->", *args, **kwargs)


# contact_list.save_to_file() # with open('list.txt') as p: # p = json.load(p) # print(p)


@app.route("/")
def index():
    pass


def main_test():
    url = "https://www.indeed.com/jobs?q=python&l=Remote"
    matches = soup_match(url, tag_name="div", attrs_pass={"class": "title"})
    # for jobTitle in matches: # if "Developer" in jobTitle.text:
    # text(default_cell_number, 'jobTitle.text')  # break # elif "Jr" in jobTitle.text:
    # print(type(matches)) for match in matches: print(match.text.strip())
    # text(default_cell_number, 'jobTitle.text')  # break


def main():
    db_print(f"{DEFAULT_PARSER}")
    wps = WebPageSet([], check_links=True, maxlen=100)
    wps.append(WebPage("https://www.google.com"))
    wps.append(WebPage("fake page"))
    wps.append("fake page")


if __name__ == "__main__":
    cgitb.enable()
    db_print(cgitb.grey("test grey font"))
    try:
        app.run()
    except:
        pass

    # start_response('200 OK', [('Content-Type', 'text/html')])
    main()

# References
""" Choice of parser

    I use BeautifulSoup to work with html documents.

        Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.

        https://www.crummy.com/software/BeautifulSoup/bs4/doc/

    According to the documentation:

        $ pip install lxml
        $ pip install html5lib

    do not use python's built in html parser (before python 3.2.2).

    - "lxml" - Use for speed (HTML)
    - "xml" - Use for speed (XML)
    - "html5lib" - Best accuracy ... very slow! (HTML)
    - "html.parser" - built in parser ... eh, it works (HTML)
    - lxml is also the ONLY supported XML parser


Latex / Bibtex

    - http://www.bibtex.org/
    - https://libguides.bates.edu/thesis_writers

Shields

    - https://shields.io/
    - https://simpleicons.org/?q=dj
    - https://docs.readthedocs.io/en/stable/badges.html
    - https://img.shields.io/badge/Django-v3-%23092E20?logo=django&color=#339933


    """
